{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The law case is that an appeal shall lie from a judgment or order passed by the one judge of the high court in exercise of original jurisdiction under Article 226 of the Constitution of India, to a division bench comprising of two judges of the same high court. However, no such appeal shall lie against an interlocutory order or against an order passed in exercise of supervisory jurisdiction under Article 227 of the Constitution of India.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, download_loader\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('.env')\n",
    "\n",
    "# downloading the loader\n",
    "\n",
    "loader = download_loader(\"SimpleDirectoryReader\")\n",
    "\n",
    "# the dataset files\n",
    "dataset_a1 = \"dataset/summary/A1\"\n",
    "dataset_a2 = \"dataset/summary/A2\"\n",
    "\n",
    "# loading the files \n",
    "dataset_a1_files = glob(f\"{dataset_a1}/*.txt\")\n",
    "dataset_a2_files = glob(f\"{dataset_a2}/*.txt\")\n",
    "\n",
    "# combining the data\n",
    "comb_data = dataset_a1_files + dataset_a2_files\n",
    "\n",
    "# creating the documents\n",
    "documents = SimpleDirectoryReader(input_files=comb_data).load_data()\n",
    "# creating indexing from documents\n",
    "index = GPTVectorStoreIndex.from_documents(documents=documents, openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# creating the query engine\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "\n",
    "# getting response for query \n",
    "\n",
    "response = query_engine.query(\"what is the law case?\")\n",
    "print(response)\n",
    "\n",
    "def get_response(query):\n",
    "    # combining the data\n",
    "    comb_data = dataset_a1_files + dataset_a2_files\n",
    "\n",
    "    # creating the documents\n",
    "    documents = SimpleDirectoryReader(input_files=comb_data).load_data()\n",
    "    # creating indexing from documents\n",
    "    index = GPTVectorStoreIndex.from_documents(documents=documents, openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "    # creating the query engine\n",
    "    query_engine = index.as_query_engine()\n",
    "    response = query_engine.query(query)\n",
    "    return response\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = GPTVectorStoreIndex.from_documents(documents=documents, openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "query_engine = index.as_query_engine()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The case of Topandas v. Firm of G. Appalaswamy (1963 Indlaw SC 379) is related to mortgage. In this case, the Supreme Court of India considered the question of whether a sitting tenant who took property by a possessory or usufructuary mortgage in his favour was liable to deliver physical possession upon redemption to the mortgagor. The Gujarat High Court decision in Patel Atmaram Nathudas v. Babubhai Keshavlal (1974 Indlaw Guj 88) is also related to mortgage. In this case, the court held that it would be unreasonable to attribute to a tenant the intention to surrender the tenancy and to invoke the sophisticated doctrine of implied surrender.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"\"\"\n",
    "show me cases related to mortgage\n",
    "\"\"\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = download_loader(\"SimpleDirectoryReader\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, ServiceContext, download_loader\n",
    "from llama_index.output_parsers import LangchainOutputParser\n",
    "from llama_index.llm_predictor import StructuredLLMPredictor\n",
    "from llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\n",
    "from llama_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT_TMPL, DEFAULT_REFINE_PROMPT_TMPL\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from glob import glob \n",
    "\n",
    "# load documents, build index\n",
    "dataset_a1 = \"dataset/summary/A1\"\n",
    "dataset_a2 = \"dataset/summary/A2\"\n",
    "\n",
    "# loading the files \n",
    "dataset_a1_files = glob(f\"{dataset_a1}/*.txt\")\n",
    "dataset_a2_files = glob(f\"{dataset_a2}/*.txt\")\n",
    "\n",
    "comb_data = dataset_a1_files + dataset_a2_files\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files = comb_data).load_data()\n",
    "index = GPTVectorStoreIndex.from_documents(documents,chunk_size=512)\n",
    "llm_predictor = StructuredLLMPredictor()\n",
    "\n",
    "# define output schema\n",
    "response_schemas = [\n",
    "        ResponseSchema(name=\"answer\", description=\"answer to the user's question\"),\n",
    "]\n",
    "\n",
    "# define output parser\n",
    "lc_output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "output_parser = LangchainOutputParser(lc_output_parser)\n",
    "\n",
    "# format each prompt with output parser instructions\n",
    "fmt_qa_tmpl = output_parser.format(DEFAULT_TEXT_QA_PROMPT_TMPL)\n",
    "fmt_refine_tmpl = output_parser.format(DEFAULT_REFINE_PROMPT_TMPL)\n",
    "qa_prompt = QuestionAnswerPrompt(fmt_qa_tmpl, output_parser=output_parser)\n",
    "refine_prompt = RefinePrompt(fmt_refine_tmpl, output_parser=output_parser)\n",
    "\n",
    "# query index\n",
    "query_engine = index.as_query_engine(\n",
    "    service_context=ServiceContext.from_defaults(\n",
    "        llm_predictor=llm_predictor\n",
    "    ),\n",
    "    text_qa_template=qa_prompt, \n",
    "    refine_template=refine_prompt, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Got invalid JSON object. Error: Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain/output_parsers/json.py:32\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m     json_obj \u001b[39m=\u001b[39m parse_json_markdown(text)\n\u001b[1;32m     33\u001b[0m \u001b[39mexcept\u001b[39;00m json\u001b[39m.\u001b[39mJSONDecodeError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain/output_parsers/json.py:25\u001b[0m, in \u001b[0;36mparse_json_markdown\u001b[0;34m(json_string)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m parsed \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(json_str)\n\u001b[1;32m     27\u001b[0m \u001b[39mreturn\u001b[39;00m parsed\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m query_engine\u001b[39m.\u001b[39;49mquery(\u001b[39m\"\u001b[39;49m\u001b[39mwhat is the law case?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/indices/query/base.py:23\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(str_or_query_bundle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m     22\u001b[0m     str_or_query_bundle \u001b[39m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 23\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_query(str_or_query_bundle)\n\u001b[1;32m     24\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/query_engine/retriever_query_engine.py:145\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    140\u001b[0m nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retriever\u001b[39m.\u001b[39mretrieve(query_bundle)\n\u001b[1;32m    141\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_event_end(\n\u001b[1;32m    142\u001b[0m     CBEventType\u001b[39m.\u001b[39mRETRIEVE, payload\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mnodes\u001b[39m\u001b[39m\"\u001b[39m: nodes}, event_id\u001b[39m=\u001b[39mretrieve_id\n\u001b[1;32m    143\u001b[0m )\n\u001b[0;32m--> 145\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_response_synthesizer\u001b[39m.\u001b[39;49msynthesize(\n\u001b[1;32m    146\u001b[0m     query_bundle\u001b[39m=\u001b[39;49mquery_bundle,\n\u001b[1;32m    147\u001b[0m     nodes\u001b[39m=\u001b[39;49mnodes,\n\u001b[1;32m    148\u001b[0m )\n\u001b[1;32m    150\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_event_end(CBEventType\u001b[39m.\u001b[39mQUERY, event_id\u001b[39m=\u001b[39mquery_id)\n\u001b[1;32m    151\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/indices/query/response_synthesis.py:178\u001b[0m, in \u001b[0;36mResponseSynthesizer.synthesize\u001b[0;34m(self, query_bundle, nodes, additional_source_nodes)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_mode \u001b[39m!=\u001b[39m ResponseMode\u001b[39m.\u001b[39mNO_TEXT:\n\u001b[1;32m    177\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_builder \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m     response_str \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_response_builder\u001b[39m.\u001b[39;49mget_response(\n\u001b[1;32m    179\u001b[0m         query_str\u001b[39m=\u001b[39;49mquery_bundle\u001b[39m.\u001b[39;49mquery_str,\n\u001b[1;32m    180\u001b[0m         text_chunks\u001b[39m=\u001b[39;49mtext_chunks,\n\u001b[1;32m    181\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_response_kwargs,\n\u001b[1;32m    182\u001b[0m     )\n\u001b[1;32m    183\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     response_str \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/indices/response/compact_and_refine.py:50\u001b[0m, in \u001b[0;36mCompactAndRefine.get_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m max_prompt \u001b[39m=\u001b[39m get_biggest_prompt([text_qa_template, refine_template])\n\u001b[1;32m     49\u001b[0m new_texts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_service_context\u001b[39m.\u001b[39mprompt_helper\u001b[39m.\u001b[39mrepack(max_prompt, text_chunks)\n\u001b[0;32m---> 50\u001b[0m response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mget_response(\n\u001b[1;32m     51\u001b[0m     query_str\u001b[39m=\u001b[39;49mquery_str, text_chunks\u001b[39m=\u001b[39;49mnew_texts, prev_response\u001b[39m=\u001b[39;49mprev_response\n\u001b[1;32m     52\u001b[0m )\n\u001b[1;32m     53\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/token_counter/token_counter.py:78\u001b[0m, in \u001b[0;36mllm_token_counter.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_llm_predict\u001b[39m(_self: Any, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m     77\u001b[0m     \u001b[39mwith\u001b[39;00m wrapper_logic(_self):\n\u001b[0;32m---> 78\u001b[0m         f_return_val \u001b[39m=\u001b[39m f(_self, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m f_return_val\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/indices/response/refine.py:52\u001b[0m, in \u001b[0;36mRefine.get_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mfor\u001b[39;00m text_chunk \u001b[39min\u001b[39;00m text_chunks:\n\u001b[1;32m     49\u001b[0m     \u001b[39mif\u001b[39;00m prev_response_obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m         \u001b[39m# if this is the first chunk, and text chunk already\u001b[39;00m\n\u001b[1;32m     51\u001b[0m         \u001b[39m# is an answer, then return it\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_give_response_single(\n\u001b[1;32m     53\u001b[0m             query_str,\n\u001b[1;32m     54\u001b[0m             text_chunk,\n\u001b[1;32m     55\u001b[0m         )\n\u001b[1;32m     56\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_refine_response_single(\n\u001b[1;32m     58\u001b[0m             prev_response_obj, query_str, text_chunk\n\u001b[1;32m     59\u001b[0m         )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/indices/response/refine.py:86\u001b[0m, in \u001b[0;36mRefine._give_response_single\u001b[0;34m(self, query_str, text_chunk, **response_kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mfor\u001b[39;00m cur_text_chunk \u001b[39min\u001b[39;00m text_chunks:\n\u001b[1;32m     82\u001b[0m     \u001b[39mif\u001b[39;00m response \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_streaming:\n\u001b[1;32m     83\u001b[0m         (\n\u001b[1;32m     84\u001b[0m             response,\n\u001b[1;32m     85\u001b[0m             formatted_prompt,\n\u001b[0;32m---> 86\u001b[0m         ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_service_context\u001b[39m.\u001b[39;49mllm_predictor\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m     87\u001b[0m             text_qa_template,\n\u001b[1;32m     88\u001b[0m             context_str\u001b[39m=\u001b[39;49mcur_text_chunk,\n\u001b[1;32m     89\u001b[0m         )\n\u001b[1;32m     90\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_prompt_and_response(\n\u001b[1;32m     91\u001b[0m             formatted_prompt, response, log_prefix\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInitial\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m         )\n\u001b[1;32m     93\u001b[0m     \u001b[39melif\u001b[39;00m response \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_streaming:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/llm_predictor/structured.py:35\u001b[0m, in \u001b[0;36mStructuredLLMPredictor.predict\u001b[0;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m# run output parser\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39mif\u001b[39;00m prompt\u001b[39m.\u001b[39moutput_parser \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[39m# TODO: return other formats\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     parsed_llm_prediction \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(prompt\u001b[39m.\u001b[39;49moutput_parser\u001b[39m.\u001b[39;49mparse(llm_prediction))\n\u001b[1;32m     36\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     parsed_llm_prediction \u001b[39m=\u001b[39m llm_prediction\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/llama_index/output_parsers/langchain.py:26\u001b[0m, in \u001b[0;36mLangchainOutputParser.parse\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Parse, validate, and correct errors programmatically.\"\"\"\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m# TODO: this object may be stringified by our upstream llmpredictor,\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m# figure out better\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m# ways to \"convert\" the object to a proper string format.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_output_parser\u001b[39m.\u001b[39;49mparse(output)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain/output_parsers/structured.py:42\u001b[0m, in \u001b[0;36mStructuredOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m     41\u001b[0m     expected_keys \u001b[39m=\u001b[39m [rs\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m rs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresponse_schemas]\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m parse_and_check_json_markdown(text, expected_keys)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain/output_parsers/json.py:34\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m     32\u001b[0m     json_obj \u001b[39m=\u001b[39m parse_json_markdown(text)\n\u001b[1;32m     33\u001b[0m \u001b[39mexcept\u001b[39;00m json\u001b[39m.\u001b[39mJSONDecodeError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m---> 34\u001b[0m     \u001b[39mraise\u001b[39;00m OutputParserException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot invalid JSON object. Error: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m expected_keys:\n\u001b[1;32m     36\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m json_obj:\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Got invalid JSON object. Error: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "query_engine.query(\"what is the law case?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document, GPTListIndex, LLMPredictor, ServiceContext, load_index_from_storage\n",
    "\n",
    "def get_llm(llm_name, model_temperature, api_key, max_tokens=256):\n",
    "    os.environ['OPENAI_API_KEY'] = api_key\n",
    "    if llm_name == \"text-davinci-003\":\n",
    "        return OpenAI(temperature=model_temperature, model_name=llm_name, max_tokens=max_tokens)\n",
    "    else:\n",
    "        return ChatOpenAI(temperature=model_temperature, model_name=llm_name, max_tokens=max_tokens)\n",
    "\n",
    "def extract_terms(documents, term_extract_str, llm_name, model_temperature, api_key):\n",
    "    llm = get_llm(llm_name, model_temperature, api_key, max_tokens=1024)\n",
    "\n",
    "    service_context = ServiceContext.from_defaults(llm_predictor=LLMPredictor(llm=llm),\n",
    "                                                   chunk_size=1024)\n",
    "\n",
    "    temp_index = GPTListIndex.from_documents(documents, service_context=service_context)\n",
    "    query_engine = temp_index.as_query_engine(response_mode=\"tree_summarize\")\n",
    "    terms_definitions = str(query_engine.query(term_extract_str))\n",
    "    terms_definitions = [x for x in terms_definitions.split(\"\\n\") if x and 'Term:' in x and 'Definition:' in x]\n",
    "    # parse the text into a dict\n",
    "    terms_to_definition = {x.split(\"Definition:\")[0].split(\"Term:\")[-1].strip(): x.split(\"Definition:\")[-1].strip() for x in terms_definitions}\n",
    "    return terms_to_definition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
